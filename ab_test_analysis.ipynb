{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, ttest_rel, sem, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate confidence interval\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "    mean = np.mean(data)\n",
    "    margin = sem(data) * t.ppf((1 + confidence) / 2., len(data) - 1)\n",
    "    return mean - margin, mean + margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to collect all results\n",
    "results_summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209caa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run t-test for selected metrics between variations and compare geographies\n",
    "def compare_metric(df, metric_name, variation_col=\"variation\", geo_col=\"geo\"):\n",
    "    print(f\"\\n Analyzing metric: **{metric_name}**\\n\" + \"-\"*50)\n",
    "\n",
    "    A = df[df[variation_col] == \"A\"][metric_name]\n",
    "    B = df[df[variation_col] == \"B\"][metric_name]\n",
    "\n",
    "    # Welch's t-test\n",
    "    t_stat, p_val = ttest_ind(A, B, equal_var=False)\n",
    "    ci_A = confidence_interval(A)\n",
    "    ci_B = confidence_interval(B)\n",
    "\n",
    "    print(f\"Welchâ€™s t-test: t = {t_stat:.4f} | p = {p_val:.4f}\")\n",
    "    print(f\"A 95% CI: {ci_A}\")\n",
    "    print(f\"B 95% CI: {ci_B}\")\n",
    "\n",
    "    if p_val < 0.05:\n",
    "        print(\"Statistically significant difference between A and B\")\n",
    "    else:\n",
    "        print(\"No statistically significant difference between A and B\")\n",
    "\n",
    "    # Append to result list\n",
    "    results_summary.append({\n",
    "        \"Metric\": metric_name,\n",
    "        \"t-statistic\": round(t_stat, 4),\n",
    "        \"p-value\": round(p_val, 4),\n",
    "        \"A mean\": round(A.mean(), 4),\n",
    "        \"B mean\": round(B.mean(), 4),\n",
    "        \"A 95% CI low\": round(ci_A[0], 4),\n",
    "        \"A 95% CI high\": round(ci_A[1], 4),\n",
    "        \"B 95% CI low\": round(ci_B[0], 4),\n",
    "        \"B 95% CI high\": round(ci_B[1], 4)})\n",
    "    \n",
    "    # Geography analysis\n",
    "\n",
    "    pivot = df.pivot(index=geo_col, columns=variation_col, values=metric_name).dropna()\n",
    "    \n",
    "    all = df[geo_col].unique().tolist()\n",
    "    paired_geos = pivot.index.tolist()\n",
    "    skipped = sorted(set(all) - set(paired_geos))\n",
    "\n",
    "    if not pivot.empty:\n",
    "        geo_t, geo_p = ttest_rel(pivot[\"A\"], pivot[\"B\"])\n",
    "        print(f\"\\n Geography-level paired t-test: p = {geo_p:.4f}\")\n",
    "        if geo_p < 0.05:\n",
    "            print(\"Statistically significant difference across geographies\")\n",
    "        else:\n",
    "            print(\"No significant difference across geographies\")\n",
    "    else:\n",
    "        print(\"\\n No geographies had both A and B for this metric.\")\n",
    "\n",
    "    if skipped:\n",
    "        print(f\"\\n Skipped geographies (missing A or B): {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78309033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and run analysis\n",
    "df = pd.read_csv(\"BigQuery_output.csv\")\n",
    "\n",
    "metrics_to_test = [\"Click-through rate\", \"Conversion rate proxy\", \"Profit per click\", \"Profit per money spend\"]\n",
    "for metric in metrics_to_test:\n",
    "    compare_metric(df, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3864cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "results_df.to_csv(\"test_summary_output.csv\", index=False)\n",
    "print(\"\\n Test results saved to 'test_summary_output.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
